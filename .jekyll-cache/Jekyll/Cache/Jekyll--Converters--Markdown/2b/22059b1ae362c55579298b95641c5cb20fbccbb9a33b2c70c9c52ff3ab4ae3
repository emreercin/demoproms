I"<p>Bayesian Networks (BNs) are graphical models that represent the combined probability distribution among a group of variables (Pearl, 1988).</p>

<p>A BN consists of two parts, the graphical structure and the parameters. The graphical structure is a directional acyclic graph. Nodes in the graph represent variables, edges represent relationships between variables. If two nodes A and B in a BA are directly connected from A to B by an edge, then A is the parent of B and B is the child of A. Each node has parameters that define its conditional probability distribution with its parents. In discrete BNs, these parameters are defined in conditional probability tables.</p>

<p>When a value is entered for any group of variables in the BN, the posterior probability distribution of other unknown variables can be calculated with inference algorithms such as a combination tree (Lauritzen and Spiegelhalter, 1988). The process of entering a value for a variable is also called entering evidence or entering an observation. With probabilistic inferences in the Bayes Network, predictive inference from parents to children, diagnostic inference from children to parents, or all-round inference between parents when the child is known can be calculated. Probabilities inference algorithms are useful for PROMs. When a value is entered into any set of questions in a PROM modeled as a BN, the posterior distributions of other questions and hidden factors can be calculated.</p>

<p>BNs can be made based on information, data, or a combination of the two. When the BN structure is made based on knowledge, the arrows in the model are determined according to the cause-effect relationships about the subject. BNs have structure learning algorithms to learn completely from data. These algorithms are divided into three groups as score-based, constraint-based and hybrid algorithms. Score-based algorithms aim to find the structure that rewards the fit of the model to the data, penalizes the model complexity, and optimizes the regulated scores (GÃ¡mez et al., 2011). Scores such as the Bayesian information criterion are suitable for this purpose. Constraint-based algorithms apply statistical tests to determine the conditional independence of the data and learn the BN structure compatible with the conditional independences detected (Le et al., 2019). Hybrid algorithms combine score-based and constraint-based principles. For example, the max-min climbing algorithm finds the graphical structure of the model without direction by applying constraint-based tests, then it aims to determine the directions using a score-based climbing algorithm (Tsamardinos et al., 2006).</p>

<p><strong><em>Bu yazÄ±nÄ±n TÃ¼rkÃ§e versiyonunu okumak iÃ§in <a href="/services/bayes_aglari/">tÄ±klayÄ±nÄ±z</a>.</em></strong></p>

<h1 id="references">References</h1>

<ul>
  <li>Pearl, J. 1988. â€œProbabilistic Reasoning in Intelligent Systemsâ€. Morgan Kaufmann.</li>
  <li>Lauritzen, S. L., Spiegelhalter, D. J. 1988. â€œLocal Computations with Probabilities on Graphical Structures and Their Application to Expert Systemsâ€. Journal of the Royal Statistical Society: Series B (Methodological), 50(2), 157â€“194.</li>
  <li>GÃ¡mez, J. A., Mateo, J. L., Puerta, J. M. 2011. â€œLearning Bayesian networks by hill climbing: efficient methods based on progressive restriction of the neighborhoodâ€. Data Mining and Knowledge Discovery, 22(1â€“2), 106â€“148.</li>
  <li>Le, T. D., Hoang, T., Li, J., Liu, L., Liu, H., Hu, S. 2019. â€œA Fast PC Algorithm for High Dimensional Causal Discovery with Multi-Core PCsâ€. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 16(5), 1483â€“1495.</li>
  <li>Tsamardinos, I., Brown, L. E., Aliferis, C. F. 2006. â€œThe max-min hill-climbing Bayesian network structure learning algorithmâ€. Machine Learning, 65(1), 31â€“78.</li>
</ul>
:ET