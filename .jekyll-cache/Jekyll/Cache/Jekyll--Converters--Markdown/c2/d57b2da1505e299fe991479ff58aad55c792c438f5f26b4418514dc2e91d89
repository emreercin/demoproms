I"°<p>One of the important constraints against the widespread use of PROMs is the need to answer too many questions and the uselessness of PROMs with incomplete inputs.</p>

<p>Bayesian Networks and their probabilistic inference algorithms provide a suitable environment for solving these two problems. No matter how many questions are answered when the Bayesian Network is built for PROMs, the posterior distributions of the measured latent factors can be calculated using probabilistic inference algorithms such as a join tree. The posterior distributions can be updated as more questions are answered. PROMs usually has multiple questions related to the same latent factor. If the uncertainty regarding a latent factor is low, that is, if the BA is certain of the latent factor estimation, asking more questions about that latent factor may not change the results. In this case, asking about another factor with more uncertainty will give more information for estimates.</p>

<p>Using an adapted questionnaire, it aims to ask the question with the highest average information gain compared to the previous answers of the patient (Madigan &amp; Almond, 1996; Plajner &amp; Vomlel, 2016).In Bayesian Networks, this process can be thought of as performing a sensitivity analysis after each question and choosing the question with the highest sensitivity as the next question. In a PROM Bayesian Network, let <strong><em>C</em></strong> be the latent factor it is intended to measure, <strong><em>x<sub>1</sub>,‚Ä¶,x<sub>a</sub></em></strong> be answers to previously answered questions <strong><em>X<sub>1</sub>,‚Ä¶,X<sub>a</sub></em></strong>, and <strong><em>X<sub>a+1</sub>,‚Ä¶,X<sub>m</sub></em></strong> be questions yet to be answered. Finally, let <strong><em>I(C;X<sub>i</sub>|x<sub>i</sub>,‚Ä¶,x<sub>a</sub>)</em></strong> be the extra information provided by the answer to question <strong><em>X<sub>i</sub></em></strong> while the answers <strong><em>x<sub>1</sub>,‚Ä¶, x<sub>a</sub></em></strong> are known. In PROMs Bayesian Networks, the question that provides the most information about the latent factor will be chosen over the previous answers.</p>

\[\underset{X{i} \in X}{\operatorname{argmax}}I(C;X{i}|x{1}...x{a})\]

<p>To calculate this, a measure of information is needed. Entropy and the amount of shared information are appropriate measures for this job. Entropy measures the amount of uncertainty in a variable. The entropy of a discrete variable <strong><em>C</em></strong> is calculated as follows.</p>

\[H(C)=-\sum_{c}P(C)\log(P(C))\]

<p>The shared amount of information for variables <strong><em>C</em></strong> and <strong><em>X</em></strong> represents the average information they provide about <strong><em>C</em></strong> when <strong><em>X</em></strong> is observed. This is the difference between the entropy of <strong><em>C</em></strong> and the conditional entropy of <strong><em>C</em></strong> to <strong><em>X</em></strong>.</p>

\[MI(C;X) = H(C) - H(C|X) = -\sum_{C}P(C)\log(P(C)) + \sum_{C, X}P(C, X)\log(P(C|X))\]

<p>If it is desired to calculate an information criterion about a group of multiple variables, the combined entropy of these variables can be used as follows.</p>

\[MI(C{1},...,C{k};X) = -\sum_{C}P(C{1},...,C{k})\log(P(C{1},...,C{k})) + \sum_{C{1},C{2},...,C{k}, X}P(C{1},...,C{k}, X)\log(P(C{1},...,C{k}|X))\]

<p>If the latent variable C is a binary variable, the expected strength of evidence may also be an appropriate measure of information (Madigan &amp; Almond, 1996). The strength of evidence <strong><em>W(L;X)</em></strong> between a latent factor C and a question item X is the logarithm of the Bayes factor.</p>

\[W(C;X) = \log(\frac{P(X|C)}{P(X|\bar{C})})\]

<p>The expected strength of evidence <strong><em>EW(C;X)</em></strong> is equal to C with the conditional mean strength of evidence. √º</p>

\[EW(C;X) = \sum_{X}W(C;X)P(X|C)\]

<p>In PROM Bayesian Networks, patient coherent questioning can be implemented by using one of the measures of common information amount or expected strength of evidence, measuring the information provided by each unanswered question according to the questions that the patient has answered before, and choosing the question with the maximum information. For such an algorithm, the maximum number of questions, the common information amount threshold or the entropy threshold can be selected as the stopping condition.</p>

<p><strong><em>Bu yazƒ±nƒ±n T√ºrk√ße versiyonunu okumak i√ßin <a href="/services/hasta-uyumlu-soru-secme/">tƒ±klayƒ±nƒ±z</a>.</em></strong></p>

<h1 id="references">References</h1>

<ul>
  <li>Madigan, D., Almond, R. G. 1996. ‚ÄúOn Test Selection Strategies for Belief Networks‚Äù. Lecture Notes in Statistics, 112, 89‚Äì98.</li>
</ul>
:ET